{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdasws import CdasWs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "cdas = CdasWs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'IMF',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'OMNI ID code for the source spacecraft for time-shifted IMF values (see OMNI documentation link for codes)'},\n",
       " {'Name': 'PLS',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'OMNI ID code for the source spacecraft  for time-shifted IP plasma values (see OMNI documentation link for codes)'},\n",
       " {'Name': 'IMF_PTS',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Number of fine time scale points in IMF averages'},\n",
       " {'Name': 'PLS_PTS',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Number of fine time scale points in plasma averages'},\n",
       " {'Name': 'percent_interp',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Percent interpolated'},\n",
       " {'Name': 'Timeshift',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Timeshift (seconds)'},\n",
       " {'Name': 'RMS_Timeshift',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'RMS Timeshift (seconds)'},\n",
       " {'Name': 'RMS_phase',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'RMS, Phase front normal (nT) '},\n",
       " {'Name': 'Time_btwn_obs',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Time between observations (seconds)'},\n",
       " {'Name': 'F',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Magnitude of avg. field vector (nT) (last currently-available OMNI B-field data Apr 11, 2023)'},\n",
       " {'Name': 'BX_GSE', 'ShortDescription': '', 'LongDescription': 'Bx (nT), GSE'},\n",
       " {'Name': 'BY_GSE', 'ShortDescription': '', 'LongDescription': 'By (nT), GSE'},\n",
       " {'Name': 'BZ_GSE', 'ShortDescription': '', 'LongDescription': 'Bz (nT), GSE'},\n",
       " {'Name': 'BY_GSM',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'By (nT), GSM, determined from post-shift GSE components'},\n",
       " {'Name': 'BZ_GSM',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Bz (nT), GSM, determined from post-shift GSE components'},\n",
       " {'Name': 'RMS_SD_B',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'RMS SD B scalar (nT)'},\n",
       " {'Name': 'RMS_SD_fld_vec',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'RMS SD field vector (nT)'},\n",
       " {'Name': 'flow_speed',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Flow Speed (km/s), GSE'},\n",
       " {'Name': 'Vx',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Vx Velocity (km/s), GSE'},\n",
       " {'Name': 'Vy',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Vy Velocity (km/s), GSE'},\n",
       " {'Name': 'Vz',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Vz Velocity (km/s), GSE'},\n",
       " {'Name': 'proton_density',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Proton density (n/cc) (last currently-available OMNI plasma data Apr 11, 2023)'},\n",
       " {'Name': 'T', 'ShortDescription': '', 'LongDescription': 'Temperature (K)'},\n",
       " {'Name': 'Pressure',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Flow pressure (nPa)'},\n",
       " {'Name': 'E',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Electric Field (mV/m)'},\n",
       " {'Name': 'Beta', 'ShortDescription': '', 'LongDescription': 'Plasma beta'},\n",
       " {'Name': 'Mach_num',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Alfven mach number'},\n",
       " {'Name': 'Mgs_mach_num',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': '1AU IP Magnetosonic mach number'},\n",
       " {'Name': 'x', 'ShortDescription': '', 'LongDescription': 'X s/c (Re), GSE'},\n",
       " {'Name': 'y', 'ShortDescription': '', 'LongDescription': 'Y s/c (Re), GSE'},\n",
       " {'Name': 'z', 'ShortDescription': '', 'LongDescription': 'Z s/c (Re), GSE'},\n",
       " {'Name': 'BSN_x',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Bow Shock Nose (Re) location, X, GSE'},\n",
       " {'Name': 'BSN_y',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Bow Shock Nose (Re) location, Y, GSE'},\n",
       " {'Name': 'BSN_z',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'Bow Shock Nose (Re) location, Z, GSE'},\n",
       " {'Name': 'AE_INDEX',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'AE - 1-minute AE-index, from WDC Kyoto (Final 1988/001-1988/182, Provisional 1990/001-2019/090)'},\n",
       " {'Name': 'AL_INDEX',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'AL - 1-minute AL-index, from WDC Kyoto (Final 1988/001-1988/182, Provisional 1990/001-2019/090)'},\n",
       " {'Name': 'AU_INDEX',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'AU - 1-minute AU-index,from WDC Kyoto  (Final 1988/001-1988/182, Provisional 1990/001-2019/090)'},\n",
       " {'Name': 'SYM_D',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'SYM/D - 1-minute SYM/D index,from WDC Kyoto (1981/001-2023/090)'},\n",
       " {'Name': 'SYM_H',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'SYM/H - 1-minute SYM/H index,from WDC Kyoto (1981/001-2023/090)'},\n",
       " {'Name': 'ASY_D',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'ASY/D - 1-minute ASY/D index,from WDC Kyoto (1981/001-2023/090)'},\n",
       " {'Name': 'ASY_H',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'ASY/H - 1-minute ASY/H index,from WDC Kyoto (1981/001-2023/090)'},\n",
       " {'Name': 'PC_N_INDEX',\n",
       "  'ShortDescription': '',\n",
       "  'LongDescription': 'PC - 1-minute Polar Cap index (North, Qaanaaq geomagnetic observatory), from DTU Space, Technical University of Denmark  (1981/001-2021/365)'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check variable names for particular dataset\n",
    "variables = cdas.get_variables('OMNI_HRO_1MIN')\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DIS-MOMS data to HDF5 file\n",
    "dates = pd.date_range('2015-07-01 ', '2023-02-01', freq='1M').strftime('%Y-%m-%d %H:%M:%S+0000').tolist()\n",
    "\n",
    "dis_df = pd.DataFrame([])\n",
    "for i in np.arange(len(dates)-1):\n",
    "    dis_df_stage = pd.DataFrame([])\n",
    "    try:\n",
    "        data = cdas.get_data('MMS1_FPI_FAST_L2_DIS-MOMS', ['mms1_dis_bulkv_gse_fast', 'mms1_dis_numberdensity_fast', 'mms1_dis_energyspectr_omni_fast'], dates[i], dates[i+1])\n",
    "    except ConnectionError:\n",
    "        print('Connection error for ' + dates[i])\n",
    "        continue\n",
    "    try:\n",
    "        dis_df_stage['Epoch_dis'] = data[1]['Epoch']\n",
    "        dis_df_stage['Vi_xgse'] = data[1]['mms1_dis_bulkv_gse_fast'][:, 0]\n",
    "        dis_df_stage['Vi_ygse'] = data[1]['mms1_dis_bulkv_gse_fast'][:, 1]\n",
    "        dis_df_stage['Vi_zgse'] = data[1]['mms1_dis_bulkv_gse_fast'][:, 2]\n",
    "        dis_df_stage['Ni'] = data[1]['mms1_dis_numberdensity_fast']\n",
    "        dis_df_stage['SW_table'] = data[1]['mms1_dis_energy_fast'][:,0] >= 190 #Solar wind energy-azimuth table starts at ~190-~210 eV\n",
    "        dis_df = dis_df.append(dis_df_stage, ignore_index=True)\n",
    "    except TypeError:\n",
    "        print('No data for ' + dates[i])\n",
    "        continue\n",
    "dis_df['Epoch_dis'] = pd.to_datetime(dis_df['Epoch_dis'], utc=True)\n",
    "#dis_df.to_hdf('../data/mms_data_test.h5', key = 'dis_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at:  2015-01-31 00:00:00+0000 2015-04-30 00:00:00+0000\n",
      "Error at:  2016-04-30 00:00:00+0000 2016-07-31 00:00:00+0000\n"
     ]
    }
   ],
   "source": [
    "#Save DES-MOMS data to HDF5 file\n",
    "dates = pd.date_range('2015-01-01', '2023-04-01', freq='3M').strftime('%Y-%m-%d %H:%M:%S+0000').tolist()\n",
    "\n",
    "des_df = pd.DataFrame([])\n",
    "for i in np.arange(len(dates)-1):\n",
    "    des_df_stage = pd.DataFrame([])\n",
    "    try:\n",
    "        data = cdas.get_data('MMS1_FPI_FAST_L2_DES-MOMS', ['mms1_des_numberdensity_fast'], dates[i], dates[i+1])\n",
    "        des_df_stage['Epoch_des'] = data[1]['Epoch']\n",
    "        des_df_stage['Ne'] = data[1]['mms1_des_numberdensity_fast']\n",
    "        des_df = des_df.append(des_df_stage, ignore_index=True)\n",
    "    except TypeError: #Throws when date range is empty OR too big\n",
    "        print('Error at: ', dates[i], dates[i+1])\n",
    "        continue\n",
    "des_df['Epoch_des'] = pd.to_datetime(des_df['Epoch_des'], utc=True)\n",
    "#des_df.to_hdf('../data/mms_data_test.h5', key='des_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save MEC data to a HDF5 file\n",
    "dates = pd.date_range('2015-01-01', '2023-04-01', freq='3M').strftime('%Y-%m-%d %H:%M:%S+0000').tolist()\n",
    "\n",
    "mec_df = pd.DataFrame([])\n",
    "for i in np.arange(len(dates)-1):\n",
    "    mec_df_stage = pd.DataFrame([])\n",
    "    try:\n",
    "        data = cdas.get_data('MMS1_MEC_SRVY_L2_EPHT89D', ['mms1_mec_r_gsm', 'mms1_mec_r_gse'], dates[i], dates[i+1])\n",
    "        mec_df_stage['Epoch_mec'] = data[1]['Epoch']\n",
    "        mec_df_stage['R_xgsm'] = data[1]['mms1_mec_r_gsm'][:, 0]\n",
    "        mec_df_stage['R_ygsm'] = data[1]['mms1_mec_r_gsm'][:, 1]\n",
    "        mec_df_stage['R_zgsm'] = data[1]['mms1_mec_r_gsm'][:, 2]\n",
    "        mec_df_stage['R_xgse'] = data[1]['mms1_mec_r_gsm'][:, 0]\n",
    "        mec_df_stage['R_ygse'] = data[1]['mms1_mec_r_gsm'][:, 1]\n",
    "        mec_df_stage['R_zgse'] = data[1]['mms1_mec_r_gsm'][:, 2]\n",
    "        mec_df_stage\n",
    "        mec_df = mec_df.append(mec_df_stage, ignore_index=True)\n",
    "    except TypeError: #Throws when date range is empty OR too big\n",
    "        print('Error at: ', dates[i], dates[i+1])\n",
    "        continue\n",
    "mec_df['Epoch_mec'] = pd.to_datetime(mec_df['Epoch_mec'], utc=True)\n",
    "mec_df.to_hdf('../data/mms_data_test.h5', key='mec_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date 8 of 366 (2022-01-08 00:00:00+0000) loaded\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6dc8b8b53539>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mfgm_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcdas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MMS1_FGM_SRVY_L2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mms1_fgm_b_gsm_srvy_l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mms1_fgm_b_gse_srvy_l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mms1_fgm_flag_srvy_l2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mfgm_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Epoch_fgm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mfgm_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Bx_gsm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mms1_fgm_b_gsm_srvy_l2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\cdasws\\__init__.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, dataset, variables, start, end, **keywords)\u001b[0m\n\u001b[0;32m   1045\u001b[0m         status_code, data_result = self.get_data_result(data_request,\n\u001b[0;32m   1046\u001b[0m                                                         \u001b[0mprogress_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m                                                         progress_user_value)\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status_code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\cdasws\\__init__.py\u001b[0m in \u001b[0;36mget_data_result\u001b[1;34m(self, data_request, progress_callback, progress_user_value)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m         response = self._session.post(url, data=data_request.json(),\n\u001b[1;32m--> 734\u001b[1;33m                                       timeout=self._timeout)\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             )\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Save FGM data to a HDF5 file\n",
    "start = '2022-01-01'\n",
    "end = '2023-01-01'\n",
    "load_freq = '1D'\n",
    "bin_freq = '5s'\n",
    "end_delt = pd.to_datetime(end) + pd.Timedelta(load_freq) #Get the last date of the date range to ensure the end is included\n",
    "dates = pd.date_range(start, end_delt, freq=load_freq).strftime('%Y-%m-%d %H:%M:%S+0000').tolist() #1 day intervals from 2015 to 2023 (inclusive)\n",
    "fgm_df = pd.DataFrame([])\n",
    "#Load the FGM data for each day and bin it\n",
    "for i in range(len(dates)-1):\n",
    "    try:\n",
    "        fgm_data = pd.DataFrame([])\n",
    "        data = cdas.get_data('MMS1_FGM_SRVY_L2', ['mms1_fgm_b_gsm_srvy_l2', 'mms1_fgm_b_gse_srvy_l2', 'mms1_fgm_flag_srvy_l2'], dates[i], dates[i+1])\n",
    "        fgm_data['Epoch_fgm'] = data[1]['Epoch']\n",
    "        fgm_data['Bx_gsm'] = data[1]['mms1_fgm_b_gsm_srvy_l2'][:, 0]\n",
    "        fgm_data['By_gsm'] = data[1]['mms1_fgm_b_gsm_srvy_l2'][:, 1]\n",
    "        fgm_data['Bz_gsm'] = data[1]['mms1_fgm_b_gsm_srvy_l2'][:, 2]\n",
    "        fgm_data['Bx_gse'] = data[1]['mms1_fgm_b_gse_srvy_l2'][:, 0]\n",
    "        fgm_data['By_gse'] = data[1]['mms1_fgm_b_gse_srvy_l2'][:, 1]\n",
    "        fgm_data['Bz_gse'] = data[1]['mms1_fgm_b_gse_srvy_l2'][:, 2]\n",
    "        fgm_data['B_flag'] = data[1]['mms1_fgm_flag_srvy_l2']\n",
    "    except TypeError: #Throws when no FGM data to load\n",
    "        fgm_data = pd.DataFrame([[np.NaN, np.NaN]], columns = ['Epoch_fgm', 'Bx_gsm'])\n",
    "        print('Date '+str(i+1)+' of '+str(len(dates))+' has no FGM data')\n",
    "        continue\n",
    "    except ValueError: #Throws when FGM data is corrupted\n",
    "        fgm_data = pd.DataFrame([[np.NaN, np.NaN]], columns = ['Epoch_fgm', 'Bx_gsm'])\n",
    "        print('Date '+str(i+1)+' of '+str(len(dates))+' has corrupted FGM data')\n",
    "        continue\n",
    "    times = pd.date_range(dates[i], dates[i+1], freq=bin_freq) #Five second intervals for the day\n",
    "    bin_subset = pd.IntervalIndex.from_arrays(times[:-1], times[1:], closed='left') #Get the bins for the current day\n",
    "    fgm_group = fgm_data.groupby(pd.cut(fgm_data['Epoch_fgm'], bin_subset)) #Bin the FGM data\n",
    "    fgm_binned_stage = fgm_group.mean() #Get the mean of the binned data\n",
    "    fgm_binned_stage['count'] = fgm_group.count()['Bx_gsm'] #Get the number of data points in each bin\n",
    "    fgm_binned_stage['Epoch_fgm'] = bin_subset.left #Add the start of the bins as the Epoch\n",
    "    fgm_df = fgm_df.append(fgm_binned_stage, ignore_index = True) #Add the binned data to the dataframe\n",
    "    print('Date '+str(i+1)+' of '+str(len(dates)-1)+' ('+dates[i]+') loaded', end='\\r')\n",
    "    del fgm_data #Delete the old data to save memory\n",
    "    del data #Delete the old data to save memory\n",
    "#fgm_df.to_hdf('../data/mms_data_test.h5', key='fgm_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a combined dataframe with all the data\n",
    "dis_df = pd.read_hdf('../data/mms_data.h5', key='dis_raw', mode = 'a')\n",
    "des_df = pd.read_hdf('../data/mms_data.h5', key='des_raw', mode = 'a')\n",
    "mec_df = pd.read_hdf('../data/mms_data.h5', key='mec_raw', mode = 'a')\n",
    "fgm_df = pd.read_hdf('../data/mms_data.h5', key='fgm_raw', mode = 'a')\n",
    "bins = pd.read_hdf('../data/mms_data.h5', key='bins', mode = 'a')\n",
    "\n",
    "bins_index = pd.IntervalIndex.from_arrays(bins['start'], bins['end'], closed='left') #Make interval index for binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished grouping mec\n",
      "Finished grouping des\n",
      "Finished grouping dis\n",
      "Finished grouping fgm\n",
      "Finished binning data\n",
      "Finished adding counts\n",
      "Finished adding Epochs\n",
      "Finished resetting index\n",
      "Finished adding region column\n"
     ]
    }
   ],
   "source": [
    "#Group the mec, des, and dis data by the bins\n",
    "verbose = True\n",
    "mec_group = mec_df.groupby(pd.cut(mec_df['Epoch_mec'], bins_index))\n",
    "if verbose: print('Finished grouping mec')\n",
    "des_group = des_df.groupby(pd.cut(des_df['Epoch_des'], bins_index))\n",
    "if verbose: print('Finished grouping des')\n",
    "dis_group = dis_df.groupby(pd.cut(dis_df['Epoch_dis'], bins_index))\n",
    "if verbose: print('Finished grouping dis')\n",
    "fgm_group = fgm_df.groupby(pd.cut(fgm_df['Epoch_fgm'], bins_index))\n",
    "if verbose: print('Finished grouping fgm')\n",
    "#Bin the mec, des, and dis data into staging dataframes\n",
    "mec_binned = mec_group.mean()\n",
    "des_binned = des_group.mean()\n",
    "dis_binned = dis_group.mean()\n",
    "fgm_binned = fgm_group.mean()\n",
    "if verbose: print('Finished binning data')\n",
    "#Add the counts in each bin\n",
    "mec_binned['count_mec'] = mec_group.count()['R_xgsm']\n",
    "des_binned['count_des'] = des_group.count()['Ne']\n",
    "dis_binned['count_dis'] = dis_group.count()['Ni']\n",
    "fgm_binned['count_fgm'] = fgm_group.count()['Bx_gsm']\n",
    "if verbose: print('Finished adding counts')\n",
    "#Get the first Epoch in each bin\n",
    "mec_binned['Epoch_mec'] = mec_group.first()['Epoch_mec']\n",
    "des_binned['Epoch_des'] = des_group.first()['Epoch_des']\n",
    "dis_binned['Epoch_dis'] = dis_group.first()['Epoch_dis']\n",
    "fgm_binned['Epoch_fgm'] = fgm_group.first()['Epoch_fgm']\n",
    "if verbose: print('Finished adding Epochs')\n",
    "#Set the index to numbers instead of the bins\n",
    "mec_binned = mec_binned.reset_index(drop = True)\n",
    "des_binned = des_binned.reset_index(drop = True)\n",
    "dis_binned = dis_binned.reset_index(drop = True)\n",
    "fgm_binned = fgm_binned.reset_index(drop = True)\n",
    "if verbose: print('Finished resetting index')\n",
    "#Add the region column\n",
    "mec_binned['region'] = bins['region'].to_numpy()\n",
    "des_binned['region'] = bins['region'].to_numpy()\n",
    "dis_binned['region'] = bins['region'].to_numpy()\n",
    "fgm_binned['region'] = bins['region'].to_numpy()\n",
    "if verbose: print('Finished adding region column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\Users\\conno\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving combined data\n"
     ]
    }
   ],
   "source": [
    "#Combine the three dataframes\n",
    "mms_data = pd.merge(mec_binned.drop(columns = 'region'), des_binned.drop(columns = 'region'), left_index = True, right_index = True)\n",
    "mms_data = pd.merge(mms_data, dis_binned.drop(columns = 'region'), left_index = True, right_index = True)\n",
    "mms_data = pd.merge(mms_data, fgm_binned.drop(columns = 'region'), left_index = True, right_index = True)\n",
    "mms_data['Epoch'] = bins['start'] #Add the start of the bins as the Epoch\n",
    "mms_data['region'] = bins['region'] #Add the region column\n",
    "#Drop any rows with NaNs\n",
    "mms_data = mms_data.dropna()\n",
    "mms_data = mms_data.reset_index(drop = True) #Reset the index\n",
    "mms_data[['R_xgsm', 'R_ygsm', 'R_zgsm', 'R_xgse', 'R_ygse', 'R_zgse']] = mms_data[['R_xgsm', 'R_ygsm', 'R_zgsm', 'R_xgse', 'R_ygse', 'R_zgse']]/6378 #Convert the R vectors to be in units of Earth radii\n",
    "#Check if MMS data was obtained with SW energy-azimuth table\n",
    "mms_data['Vi_ygse'][mms_data['SW_table']] += -10.7 #Subtract 10.7 km/s to Vy_gse if MMS data was obtained with SW energy-azimuth table\n",
    "mms_data['Vi_ygse'][mms_data['SW_table']] += -29.0 #Subtract 29.0 km/s to Vy_gse if MMS data was not obtained with SW energy-azimuth table\n",
    "#Save the combined data\n",
    "#mms_data.to_hdf('../data/mms_data.h5', key = 'targets', mode = 'a')\n",
    "print('Finished saving combined data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sw_lib as sw\n",
    "start = pd.to_datetime('2022-01-01')\n",
    "end = pd.to_datetime('2023-01-01')\n",
    "dates = pd.date_range(start, end + pd.Timedelta('1D'), freq='1D').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "date = start #Initialize the date\n",
    "ol_df = pd.DataFrame([]) #Initialize the dataframe\n",
    "for i in np.arange(len(dates)-1):\n",
    "    staging_df = pd.DataFrame([]) #Initialize the staging dataframe\n",
    "    try:\n",
    "        data = cdas.get_data('MMS1_FPI_FAST_L2_DIS-DIST', ['mms1_dis_dist_fast'], dates[i], dates[i+1]) #Get the data\n",
    "        dist_arr = np.asarray(data[1]['mms1_dis_dist_fast']) #Pull the 3D distribution\n",
    "        label, probability = sw.olshevsky_class(dist_arr) #Classify the full array\n",
    "        p_arr = sw.translate_probabilities(probability) #Store the class probabilities\n",
    "        f_arr = sw.translate_labels(label, classifier = 'olshevsky') #Region flags (see translate_labels)\n",
    "        staging_df['Epoch'] = data[1]['Epoch'] #Get the time array\n",
    "        staging_df['region'] = f_arr #Get the region array\n",
    "        staging_df['MSP_p'] = p_arr[:, 0] #Get the MSP probabilities\n",
    "        staging_df['MSH_p'] = p_arr[:, 1] #Get the MSH probabilities\n",
    "        staging_df['SW_p'] = p_arr[:, 2] #Get the SW probabilities\n",
    "        staging_df['IF_p'] = p_arr[:, 3] #Get the IF probabilities\n",
    "        ol_df = ol_df.append(staging_df, ignore_index = True) #Append this day to the full dataframe\n",
    "    except:\n",
    "        print('Classification failed for date: '+date[i]) #Throw a warning\n",
    "        continue #Skip this day\n",
    "ol_df.to_hdf('../data/ol_class_2022', key='ol_class_2022', mode = 'a') #Save our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "store = pd.HDFStore('../data/sw_data.h5', mode = 'a')\n",
    "store['sw_table_list'].to_hdf('sw_table_list.h5', key='sw_table_list', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct the old dataset (while the new one goes through growing pains)\n",
    "mms_data_old = pd.read_hdf('../data/sw_data.h5', key = 'mms_target') #Load the MMS data binned to 100s intervals\n",
    "des_raw = pd.read_hdf('../data/mms_data.h5', key='des_raw') #Load the DES data\n",
    "sw_table = pd.read_hdf('../data/sw_table_list.h5', key = 'sw_table_list', mode = 'a') #List of files obtained with SW energy-azimuth table\n",
    "\n",
    "mms_data_old['Epoch'] = pd.to_datetime(mms_data_old['time'], unit = 's', origin='unix', utc = True) #Convert the timestamp to a datetime object\n",
    "bins = pd.IntervalIndex.from_arrays(mms_data_old['Epoch'], mms_data_old['Epoch'] + pd.Timedelta('100s'), closed = 'left') #Get the bins for the MMS data\n",
    "des_group = des_raw.groupby(pd.cut(des_raw['Epoch_des'], bins))\n",
    "des_stage = des_group.mean()\n",
    "des_stage.index = mms_data_old.index #Reset the index to be sequential\n",
    "mms_data_old['Ne'] = des_stage['Ne']\n",
    "\n",
    "#Check if MMS data was obtained with SW energy-azimuth table\n",
    "table_bool = mms_data_old['Epoch'].round('2H').isin(sw_table) #Boolean array of MMS data that was obtained with SW energy-azimuth table\n",
    "mms_data_old['Vy_gse'][(table_bool)&(mms_data_old['regid'] == 2)] += -10.7 #Subtract 10.7 km/s to Vy_gse if MMS data was obtained with SW energy-azimuth table\n",
    "mms_data_old['Vy_gse'][(~table_bool)&(mms_data_old['regid'] == 2)] += -29.0 #Subtract 29.0 km/s to Vy_gse if MMS data was not obtained with SW energy-azimuth table\n",
    "mms_data_old.rename(columns = {'Bx_gsm' : 'B_xgsm', 'By_gsm' : 'B_ygsm', 'Bz_gsm' : 'B_zgsm', \n",
    "                               'Vx_gse' : 'Vi_xgse', 'Vy_gse' : 'Vi_ygse', 'Vz_gse' : 'Vi_zgse', \n",
    "                               'regid' : 'region', 'Px_gse' : 'R_xgse', 'Py_gse' : 'R_ygse', 'Pz_gse' : 'R_zgse'}, inplace = True)\n",
    "#mms_data_old.to_hdf('../data/sw_data.h5', key = 'mms_target_corrected2', mode = 'a') #Save the corrected MMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Wind MFI data to a HDF5 file\n",
    "dates = pd.date_range('2015-01-01', '2023-04-01', freq='3M').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "mfi_df = pd.DataFrame([])\n",
    "for i in np.arange(len(dates)-1):\n",
    "    mfi_df_stage = pd.DataFrame([])\n",
    "    try:\n",
    "        data = cdas.get_data('WI_H0_MFI', ['BGSM', 'PGSE'], dates[i], dates[i+1])\n",
    "        mfi_df_stage['Epoch'] = data[1]['Epoch']\n",
    "        mfi_df_stage['R_xgse'] = data[1]['PGSE'][:, 0]\n",
    "        mfi_df_stage['R_ygse'] = data[1]['PGSE'][:, 1]\n",
    "        mfi_df_stage['R_zgse'] = data[1]['PGSE'][:, 2]\n",
    "        mfi_df_stage['B_xgsm'] = data[1]['BGSM'][:, 0]\n",
    "        mfi_df_stage['B_ygsm'] = data[1]['BGSM'][:, 1]\n",
    "        mfi_df_stage['B_zgsm'] = data[1]['BGSM'][:, 2]\n",
    "        mfi_df = mfi_df.append(mfi_df_stage, ignore_index=True)\n",
    "    except TypeError: #Throws when date range is empty OR too big\n",
    "        print('Error at: ', dates[i], dates[i+1])\n",
    "        continue\n",
    "mfi_df['Epoch'] = pd.to_datetime(mfi_df['Epoch'], utc=True)\n",
    "#Set B values to nan if they are equal to the fill value of -1e31\n",
    "mfi_df['B_xgsm'].where(mfi_df['B_xgsm'] > -1e30, np.nan, inplace=True)\n",
    "mfi_df['B_ygsm'].where(mfi_df['B_ygsm'] > -1e30, np.nan, inplace=True)\n",
    "mfi_df['B_zgsm'].where(mfi_df['B_zgsm'] > -1e30, np.nan, inplace=True)\n",
    "#Set R values to nan if they are equal to the fill value of -1e31\n",
    "mfi_df['R_xgse'].where(mfi_df['R_xgse'] > -1e30, np.nan, inplace=True)\n",
    "mfi_df['R_ygse'].where(mfi_df['R_ygse'] > -1e30, np.nan, inplace=True)\n",
    "mfi_df['R_zgse'].where(mfi_df['R_zgse'] > -1e30, np.nan, inplace=True)\n",
    "#mfi_df.to_hdf('../data/wind_data_test.h5', key='mfi_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Wind SWE data to a HDF5 file\n",
    "dates = pd.date_range('2015-01-01', '2023-04-01', freq='3M').strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "swe_df = pd.DataFrame([])\n",
    "for i in np.arange(len(dates)-1):\n",
    "    swe_df_stage = pd.DataFrame([])\n",
    "    try:\n",
    "        data = cdas.get_data('WI_K0_SWE', ['Np', 'V_GSE', 'THERMAL_SPD', 'QF_V', 'QF_Np'], dates[i], dates[i+1])\n",
    "        swe_df_stage['Epoch'] = data[1]['Epoch']\n",
    "        swe_df_stage['Ni'] = data[1]['Np']\n",
    "        swe_df_stage['Vi_xgse'] = data[1]['V_GSE'][:, 0]\n",
    "        swe_df_stage['Vi_ygse'] = data[1]['V_GSE'][:, 1]\n",
    "        swe_df_stage['Vi_zgse'] = data[1]['V_GSE'][:, 2]\n",
    "        swe_df_stage['Vth'] = data[1]['THERMAL_SPD']\n",
    "        swe_df_stage['vflag'] = data[1]['QF_V']\n",
    "        swe_df_stage['niflag'] = data[1]['QF_Np']\n",
    "        swe_df = swe_df.append(swe_df_stage, ignore_index=True)\n",
    "    except TypeError: #Throws when date range is empty OR too big\n",
    "        print('Error at: ', dates[i], dates[i+1])\n",
    "        continue\n",
    "swe_df['Epoch'] = pd.to_datetime(swe_df['Epoch'], utc=True)\n",
    "#Remove erroneous Epochs outside downloaded date range (due to CDAS bug)\n",
    "swe_df['Epoch'].where(swe_df['Epoch'] >= pd.to_datetime(dates[0], utc=True), np.nan, inplace=True)\n",
    "swe_df['Epoch'].where(swe_df['Epoch'] <= pd.to_datetime(dates[-1], utc=True), np.nan, inplace=True)\n",
    "#Remove rows with nan Epochs and reset the index\n",
    "swe_df.dropna(subset=['Epoch'], inplace=True)\n",
    "swe_df.reset_index(drop=True, inplace=True)\n",
    "#Set Ni values to nan if they are equal to the fill value of -1e31\n",
    "swe_df['Ni'].where(swe_df['Ni'] > -1e30, np.nan, inplace=True)\n",
    "#Set Vi values to nan if they are equal to the fill value of -1e31\n",
    "swe_df['Vi_xgse'].where(swe_df['Vi_xgse'] > -1e30, np.nan, inplace=True)\n",
    "swe_df['Vi_ygse'].where(swe_df['Vi_ygse'] > -1e30, np.nan, inplace=True)\n",
    "swe_df['Vi_zgse'].where(swe_df['Vi_zgse'] > -1e30, np.nan, inplace=True)\n",
    "#Set Vth values to nan if they are equal to the fill value of -1e31\n",
    "swe_df['Vth'].where(swe_df['Vth'] > -1e30, np.nan, inplace=True)\n",
    "#Set vflag values to nan if they are equal to the fill value of -2147483648\n",
    "swe_df['vflag'].where(swe_df['vflag'] > -2147483648, np.nan, inplace=True)\n",
    "#Set niflag values to nan if they are equal to the fill value of -2147483648\n",
    "swe_df['niflag'].where(swe_df['niflag'] > -2147483648, np.nan, inplace=True)\n",
    "#swe_df.to_hdf('../data/wind_data_test.h5', key='swe_raw', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Interpolation Loop 96.53% complete\r"
     ]
    }
   ],
   "source": [
    "def closest_argmin(A, B):\n",
    "    '''\n",
    "    Helper function that returns indices of elements in array B closest to each element in array A.\n",
    "    '''\n",
    "    L = B.size\n",
    "    sidx_B = B.argsort()\n",
    "    sorted_B = B[sidx_B]\n",
    "    sorted_idx = np.searchsorted(sorted_B, A)\n",
    "    sorted_idx[sorted_idx==L] = L-1\n",
    "    mask = (sorted_idx > 0) & \\\n",
    "    ((np.abs(A - sorted_B[sorted_idx-1]) < np.abs(A - sorted_B[sorted_idx])) )\n",
    "    return sidx_B[sorted_idx-mask]\n",
    "\n",
    "#Combine the two dataframes\n",
    "swe_df = pd.read_hdf('../data/wind_data_test.h5', key='swe_raw')\n",
    "mfi_df = pd.read_hdf('../data/wind_data_test.h5', key='mfi_raw')\n",
    "\n",
    "#Drop rows with dips in the Epoch\n",
    "swe_df_new = swe_df.copy()\n",
    "for i in np.arange(1,len(swe_df)):  #First we hunt dips in the data\n",
    "    delt = (swe_df['Epoch'][i] - swe_df['Epoch'][i-1]).total_seconds()\n",
    "    if (delt < 0): #If the time difference is negative, we have a dip!\n",
    "        swe_df_new.drop(index=i, inplace=True) #Drop the row\n",
    "    print('Dip Loop ' + str(100*i/len(swe_df))[0:5] + '% complete', end='\\r')\n",
    "\n",
    "max_allowable_delt = 100 #Maximum allowable time difference between consecutive entries\n",
    "swe_df_new.reset_index(drop=True, inplace=True) #Reset the index\n",
    "for i in np.arange(1,len(swe_df_new)):  #Then we hunt for large time differences\n",
    "    delt = (swe_df_new['Epoch'][i] - swe_df_new['Epoch'][i-1]).total_seconds()\n",
    "    if (delt > max_allowable_delt*2): #If the time difference is too large, we have a gap!\n",
    "        rows_stage = pd.DataFrame([]) #Initialize the staging dataframe\n",
    "        rows_stage['Epoch'] = pd.date_range(swe_df_new['Epoch'][i-1], swe_df_new['Epoch'][i], freq='100s')[1:] #Get the new Epochs\n",
    "        swe_df_new = swe_df_new.append(rows_stage, ignore_index=True) #Append the rows\n",
    "    print('Time Interpolation Loop ' + str(100*i/len(swe_df_new))[0:5] + '% complete', end='\\r')\n",
    "swe_df_new['interp_flag'] = swe_df_new.isna().any(axis=1) #Get the rows with NaNs and flag them as interpolated\n",
    "swe_df_new = swe_df_new.sort_values(by=['Epoch']) #Sort the dataframe by Epoch\n",
    "swe_df_new = swe_df_new.reset_index(drop=True) #Reset the index\n",
    "\n",
    "#Find elements in mfi_df closest to each Epoch in swe_df\n",
    "args = closest_argmin(swe_df_new['Epoch'].to_numpy(), mfi_df['Epoch'].to_numpy())\n",
    "mfi_df_stage = mfi_df.iloc[args] #Get the closest elements\n",
    "mfi_df_stage.index = swe_df_new.index #Reset the index to be sequential\n",
    "swe_df_new = pd.merge(swe_df_new, mfi_df_stage, left_index=True, right_index=True) #Merge the dataframes\n",
    "swe_df_new.rename(columns={'Epoch_x': 'Epoch', 'Epoch_y': 'Epoch_mfi'}, inplace=True) #Rename the Epoch columns\n",
    "swe_df_new['flag'] = swe_df_new.isna().any(axis=1) #Get the rows with NaNs and flag them as interpolated\n",
    "\n",
    "#Interpolate the NaNs\n",
    "swe_df_new['Ni'] = swe_df_new['Ni'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['Vi_xgse'] = swe_df_new['Vi_xgse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['Vi_ygse'] = swe_df_new['Vi_ygse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['Vi_zgse'] = swe_df_new['Vi_zgse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['Vth'] = swe_df_new['Vth'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['vflag'] = swe_df_new['vflag'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['niflag'] = swe_df_new['niflag'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['R_xgse'] = swe_df_new['R_xgse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['R_ygse'] = swe_df_new['R_ygse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['R_zgse'] = swe_df_new['R_zgse'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['B_xgsm'] = swe_df_new['B_xgsm'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['B_ygsm'] = swe_df_new['B_ygsm'].interpolate(method='linear', axis=0)\n",
    "swe_df_new['B_zgsm'] = swe_df_new['B_zgsm'].interpolate(method='linear', axis=0)\n",
    "\n",
    "swe_df_new['Epoch_delta'] = swe_df_new['Epoch'] - swe_df_new['Epoch_mfi'] #Get the difference between the Epochs\n",
    "mec_data = pd.read_hdf('../data/mms_data.h5', key='mec_raw', mode = 'a')\n",
    "mec_data.rename(columns={'R_xgse' : 'target_R_xgse', 'R_ygse' : 'target_R_ygse', 'R_zgse' : 'target_R_zgse'}, inplace=True)\n",
    "mec_data.rename(columns={'R_xgsm' : 'target_R_xgsm', 'R_ygsm' : 'target_R_ygsm', 'R_zgsm' : 'target_R_zgsm'}, inplace=True)\n",
    "mec_data[['target_R_xgse', 'target_R_ygse', 'target_R_zgse']] = mec_data[['target_R_xgse', 'target_R_ygse', 'target_R_zgse']]/6378 #Convert the R vectors to be in units of Earth radii\n",
    "mec_data[['target_R_xgsm', 'target_R_ygsm', 'target_R_zgsm']] = mec_data[['target_R_xgsm', 'target_R_ygsm', 'target_R_zgsm']]/6378 #Convert the R vectors to be in units of Earth radii\n",
    "args = closest_argmin(swe_df_new['Epoch'].to_numpy(), mec_data['Epoch_mec'].to_numpy())\n",
    "mec_data_stage = mec_data.iloc[args] #Get the closest elements\n",
    "mec_data_stage.index = swe_df_new.index #Reset the index to be sequential\n",
    "swe_df_new = pd.merge(swe_df_new, mec_data_stage, left_index=True, right_index=True) #Merge the dataframes\n",
    "#swe_df_new.to_hdf('../data/wind_data_test.h5', key='wind_combined', mode = 'a') #Save the combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sw_lib as sw\n",
    "wind_2015 = pd.read_csv('../data/omni_data/wind_ascii/2015wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2016 = pd.read_csv('../data/omni_data/wind_ascii/2016wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2017 = pd.read_csv('../data/omni_data/wind_ascii/2017wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2018 = pd.read_csv('../data/omni_data/wind_ascii/2018wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2019 = pd.read_csv('../data/omni_data/wind_ascii/2019wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2020 = pd.read_csv('../data/omni_data/wind_ascii/2020wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2021 = pd.read_csv('../data/omni_data/wind_ascii/2021wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_2022 = pd.read_csv('../data/omni_data/wind_ascii/2022wind.txt', delim_whitespace=True, names = sw.WIND_ASCII_COLS)\n",
    "wind_full = wind_2015.append([wind_2016, wind_2017, wind_2018, wind_2019, wind_2020, wind_2021, wind_2022], ignore_index=True)\n",
    "wind_full = wind_full.drop_duplicates()\n",
    "wind_full.index = np.arange(len(wind_full)) #reindex\n",
    "date = []\n",
    "for i in np.arange(len(wind_full)):\n",
    "    date.append(pd.to_datetime(str(int(wind_full['year'][i]))+'-'+str(int(wind_full['doy'][i]))+'-'+str(int(wind_full['hour'][i]))+'-'+str(int(wind_full['minute'][i])), format = '%Y-%j-%H-%M', utc=True))\n",
    "wind_full['Epoch'] = date\n",
    "wind_full.to_hdf('../data/wind_data.h5', key='wind_omni_bs', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMNI Propagation 0.001% Complete\r"
     ]
    }
   ],
   "source": [
    "import sw_lib as sw\n",
    "mms_data = pd.read_hdf('../data/mms_data.h5', key='targets', mode = 'a')\n",
    "wind_data = pd.read_hdf('../data/wind_data.h5', key='wind_omni_bs', mode = 'a')\n",
    "\n",
    "omni_ind = np.zeros(len(mms_data))\n",
    "shift = np.zeros(len(mms_data))\n",
    "\n",
    "omni_time = (wind_data['Epoch'] - pd.Timestamp(\"1970-01-01 00:00:00+0000\")) // pd.Timedelta('1s') #Convert to seconds since epoch\n",
    "\n",
    "nx = wind_data['Phase_n_x'] #Phase front normal vector\n",
    "ny = wind_data['Phase_n_y']\n",
    "nz = wind_data['Phase_n_z']\n",
    "\n",
    "Vx = wind_data['VX_GSE'] #Solar wind velocity\n",
    "Vy = wind_data['VY_GSE']\n",
    "Vz = wind_data['VZ_GSE']\n",
    "\n",
    "denominator = (nx * Vx) + (ny * Vy) + (nz * Vz) #Denominator of time delay function\n",
    "\n",
    "Rox = wind_data['BSN_X'] * sw.EARTH_RADIUS #Wind position (Convert from Re to km)\n",
    "Roy = wind_data['BSN_Y'] * sw.EARTH_RADIUS\n",
    "Roz = wind_data['BSN_Z'] * sw.EARTH_RADIUS\n",
    "\n",
    "for i in np.arange(len(mms_data)):\n",
    "    mms_time = (mms_data['Epoch'][i]- pd.Timestamp(\"1970-01-01 00:00:00+0000\")) // pd.Timedelta('1s') #Convert to seconds since epoch\n",
    "    cut_bool = wind_data['Epoch'].between(mms_data['Epoch'][i] - pd.Timedelta('1D'), mms_data['Epoch'][i]) #Boolean array of omni data within 1 day on either side of MMS data\n",
    "    min_ind = cut_bool[cut_bool].index.min() #Find the lowest index so we can store the right one later\n",
    "\n",
    "    Rdx = mms_data['R_xgse'][i] * sw.EARTH_RADIUS #MMS position (Convert from Re to km)\n",
    "    Rdy = mms_data['R_ygse'][i] * sw.EARTH_RADIUS\n",
    "    Rdz = mms_data['R_zgse'][i] * sw.EARTH_RADIUS\n",
    "\n",
    "    delta_t = (nx[cut_bool] * (Rdx - Rox[cut_bool]) +  ny[cut_bool] * (Rdy - Roy[cut_bool]) + nz[cut_bool] * (Rdz - Roz[cut_bool])) / denominator[cut_bool] #OMNI algorithm time delay function\n",
    "    try:\n",
    "        omni_ind[i] = np.argmin(np.abs(omni_time[cut_bool] + delta_t - mms_time)) + min_ind #which piece of omni_data corresponds to this adjusted time?\n",
    "        shift[i] = np.min(np.abs(omni_time[cut_bool] + delta_t - mms_time))\n",
    "    except ValueError: #Throws when there's no data in omni_data within 1 day on either side of MMS data\n",
    "        omni_ind[i] = 0 #Unfortunately nan values don't work with pandas indexing. Look for nan timeshifts to drop data later\n",
    "        shift[i] = np.nan #Unfortunately nan values don't work with pandas indexing. Look for nan timeshifts to drop data later\n",
    "    print('OMNI Propagation '+str(100*i/len(mms_data))[0:5]+'% Complete', end='\\r')\n",
    "omni_data_shift = wind_data.loc[omni_ind]\n",
    "omni_data_shift['timeshift'] = shift\n",
    "omni_data_shift.reset_index(drop=True, inplace=True)\n",
    "#omni_data_shift.to_hdf('../data/wind_data.h5', key='wind_omni_shift', mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\function_base.py:804: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, order=order, subok=subok, copy=True)\n"
     ]
    }
   ],
   "source": [
    "import ks_custom as ksc\n",
    "#Split up and scale the datasets\n",
    "in_keys = ['B_xgsm', 'B_ygsm', 'B_zgsm', 'Vi_xgse', 'Vi_ygse', 'Vi_zgse', 'Ni', 'Vth', 'R_xgse', 'R_ygse', 'R_zgse', 'target_R_xgse', 'target_R_ygse', 'target_R_zgse'] #Wind data keys to include in input dataset\n",
    "tar_keys = ['B_xgsm', 'B_ygsm', 'B_zgsm', 'Vi_xgse', 'Vi_ygse', 'Vi_zgse', 'Ne'] #Targets from MMS dataset to match\n",
    "window = 70 #Input window length (100s units)\n",
    "stride = 16 #Stride away from target (100s units)\n",
    "inter_frac = 0.15 #How many interpolated points in a row to tolerate\n",
    "flag = 'tdelt' #Whether to cut inputs as a percent of total or length of maximum interpolated stretch of data\n",
    "ds = ksc.load_dataset('../data/mms_data.h5', '../data/wind_data.h5', 'sw', in_keys, tar_keys, split_frac=0.2, window=window, stride=stride, inter_frac=int(inter_frac*window), flag=flag, conesize = np.pi/2)\n",
    "mms_data = pd.read_hdf('../data/mms_data.h5', key = 'targets', mode = 'a')\n",
    "mms_data = ksc.nightside_cut(mms_data, conesize = np.pi/2) #Cut out the nightside data since classification is unreliable there\n",
    "mms_data = mms_data[mms_data['Vi_xgse'] <= -250] #Cut out erroneous super slow solar wind (maybe it's the magnetosheath?)\n",
    "mms_sw = mms_data.drop(mms_data[mms_data['region']!=2].index).dropna() #Solar Wind Data\n",
    "mms_sh = mms_data.drop(mms_data[mms_data['region']!=1].index).dropna() #Magnetosheath Data\n",
    "mms_cut = mms_sw[ds.ds_mask]\n",
    "mms_train = mms_cut.iloc[ds.inds_train]\n",
    "mms_test = mms_cut.iloc[ds.inds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
