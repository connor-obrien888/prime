{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf329a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from cdasws import CdasWs\n",
    "\n",
    "DATAPATH = \"~/data/\" # Base data directory\n",
    "cdas = CdasWs() # Open connection to CDAS\n",
    "\n",
    "def load_range(dataset_name, var_list, start_date, end_date, load_freq = '1MS', time_name = 'Epoch', resample_freq = None, interp_freq = None, verbose = False, retries = 5):\n",
    "    dates = pd.date_range(start_date, end_date, freq=load_freq).strftime('%Y-%m-%d %H:%M:%S+0000').tolist()\n",
    "    df_list = []\n",
    "    for i in np.arange(len(dates)-1):\n",
    "        if verbose:\n",
    "            print(f\"Processing {dates[i]}\")\n",
    "        loaded = False\n",
    "        tries = 0\n",
    "        while loaded == False:\n",
    "            if tries > retries:\n",
    "                break\n",
    "            try:\n",
    "                df_stage = cdf_to_df_remote(dataset_name, var_list, dates[i], dates[i+1], time_name = time_name)\n",
    "                loaded = True\n",
    "                tries = 0\n",
    "            except ConnectionError:\n",
    "                tries += 1\n",
    "                print(f'Connection error for {dates[i]}. Retrying {tries} of {retries} times.')\n",
    "            except TypeError:\n",
    "                print(f'No data for {dates[i]}')\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(f'Corrupted data for {dates[i]}')\n",
    "                break\n",
    "        if loaded == True: # Did we successfully load the data in the above while loop?\n",
    "            if resample_freq is not None: # Do we want to downsample the loaded data? (Useful for high-cadence data like FGM)\n",
    "                times = pd.date_range(dates[i], dates[i+1], freq=resample_freq) #Make new time intervals\n",
    "                bin_subset = pd.IntervalIndex.from_arrays(times[:-1], times[1:], closed='left') #Create binlike object from times\n",
    "                group = df_stage.groupby(pd.cut(df_stage[time_name], bin_subset), observed = False) #Bin the data\n",
    "                binned_df_stage = group.mean() #Get the mean of the binned data\n",
    "                binned_df_stage['count'] = group.count()[time_name] #Get the number of data points in each bin\n",
    "                binned_df_stage[time_name] = bin_subset.left #Add the start of the bins as the Epoch\n",
    "                df_list.append(binned_df_stage)\n",
    "                del df_stage # Explicitly remove the raw data dataframe from memory in case it's HUGE (like FGM data!)\n",
    "            elif interp_freq is not None: # Do we want to inerpolate the loaded data? (Useful for low-cadence data like SWE)\n",
    "                times = pd.date_range(dates[i], dates[i+1], freq=interp_freq) #Make new time intervals\n",
    "                df_interp = pd.DataFrame([])\n",
    "                df_interp[time_name] = times[:-1] # For some reason date_range includes the last time\n",
    "                for var in df_stage.columns:\n",
    "                    if (var == time_name): # Skip the time column because we already have it\n",
    "                        continue\n",
    "                    df_interp[var] = np.interp(df_interp[time_name], df_stage[time_name], df_stage[var])\n",
    "                df_list.append(df_interp)\n",
    "            else: # If not resampling, we just append dis_df_stage directly\n",
    "                df_list.append(df_stage)\n",
    "        gc.collect() # We're running out of memory when using this function sometimes, so maybe this will help\n",
    "    dataframe = pd.concat(df_list, ignore_index = True)\n",
    "    return dataframe\n",
    "\n",
    "def cdf_to_df_remote(dataset_name, var_list, start_date, end_date, time_name = 'Epoch'):\n",
    "    dataframe = pd.DataFrame([])\n",
    "    data = cdas.get_data(dataset_name, var_list, start_date, end_date)\n",
    "    dataframe[time_name] = data[1][time_name] # This one is special because it's not in the var_list\n",
    "    for var in var_list:\n",
    "        if (var == 'mms1_dis_energyspectr_omni_fast'): \n",
    "            # If you need to handle more than one special variable, you should define a global with all special variables and run the if\n",
    "            # statement with is in that list, then call a function handle_special_variable here that handles all the special cases.\n",
    "            dataframe['SW_table'] = (data[1]['mms1_dis_energy_fast'][:,0] >= 190) #Solar wind energy-azimuth table starts at ~190-~210 eV\n",
    "        if (data[1][var].ndim == 2): # We gotta handle vector data differently because the ending structure must be 2D\n",
    "            for i in range(data[1][var].shape[-1]):\n",
    "                dataframe[var+'_'+str(i)] = data[1][var][:,i]\n",
    "        elif (data[1][var].ndim == 1):\n",
    "            dataframe[var] = data[1][var]\n",
    "        else:\n",
    "            raise ValueError(f\"Variable {var} in {dataset_name} has {data[1][var].ndim} dimensions (perhaps it is a distribution function or a constant).\")\n",
    "    dataframe[time_name] = pd.to_datetime(dataframe[time_name], utc=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from MMS and Wind\n",
    "save_raw = True # Whether to save the raw data in the mms_data.h5 and wind_data.h5 HDF file\n",
    "# datestrs = ['2015-09-01 00:00:00+00:00', '2025-01-01 00:00:00+00:00']\n",
    "datestrs = ['2015-09-01 00:00:00+00:00', '2025-01-01 00:00:00+00:00']\n",
    "fpi_i_var_list = [\n",
    "    'mms1_dis_bulkv_gse_fast', # V_gse vector for ions in km/s\n",
    "    'mms1_dis_numberdensity_fast',  # n_i in cm**-3\n",
    "    'mms1_dis_energyspectr_omni_fast', # Used to determine if SW energy-azimuth table is active (SW_table bool). Not included directly in outputs\n",
    "    'mms1_dis_temppara_fast', # T_i parallel to B in eV\n",
    "    'mms1_dis_tempperp_fast', # T_i perpendicular to B in eV\n",
    "]\n",
    "fpi_e_var_list = [\n",
    "    'mms1_des_numberdensity_fast', # n_e in cm**-3\n",
    "    'mms1_des_temppara_fast', # T_e parallel to B in eV\n",
    "    'mms1_des_tempperp_fast', # T_i parallel to B in eV\n",
    "    'mms1_des_bulkv_gse_fast', # V_gse vector for electrons in km/s\n",
    "]\n",
    "mec_var_list = [\n",
    "    'mms1_mec_r_gsm', # SC position vector (GSM) in km\n",
    "    'mms1_mec_r_gse', # SC position vector (GSE) in km\n",
    "]\n",
    "fgm_var_list = [\n",
    "    'mms1_fgm_b_gsm_srvy_l2', # DC B field vector (GSM) in nT \n",
    "    'mms1_fgm_b_gse_srvy_l2', # DC B field vector (GSE) in nT\n",
    "    'mms1_fgm_flag_srvy_l2', # B field quality flag (not sure why this is here)\n",
    "]\n",
    "mfi_var_list = [\n",
    "    'BGSM', # DC B field vector (GSM) in nT\n",
    "    'BGSE', # DC B field vector (GSE) in nT\n",
    "    'PGSM', # Wind position vector (GSM) in RE\n",
    "    'PGSE', # Wind position vector (GSE) in RE\n",
    "]\n",
    "swe_var_list = [\n",
    "    'Np', # Proton density in cm**-3\n",
    "    'V_GSE', # Ion flow velocity vector (GSE) in km/s\n",
    "    'V_GSM', # Ion flow velocity vecotr (GSM) in km/s\n",
    "    'THERMAL_SPD', # Proton thermal speed in km/s\n",
    "    'Pressure', # Dynamic pressure in nPa, technically calculable from the others but why bother\n",
    "    'QF_V', # Velocity quality flag\n",
    "    'QF_Np', # Proton density quality flag\n",
    "]\n",
    "fpi_i_dataset = 'MMS1_FPI_FAST_L2_DIS-MOMS' # Level 2 fast moments\n",
    "fpi_e_dataset = 'MMS1_FPI_FAST_L2_DES-MOMS' # Level 2 fast moments\n",
    "mec_dataset = 'MMS1_MEC_SRVY_L2_EPHT89D' # Level 2 survey with EPHT89D field model\n",
    "fgm_dataset = 'MMS1_FGM_SRVY_L2' # Level 2 survey\n",
    "mfi_dataset = 'WI_H0_MFI' # Key parameter B from Wind\n",
    "swe_dataset = 'WI_K0_SWE' # Key parameter plasma from Wind\n",
    "fpi_i_data = load_range(fpi_i_dataset, fpi_i_var_list, datestrs[0], datestrs[1], load_freq='1MS', resample_freq='1min')\n",
    "fpi_e_data = load_range(fpi_e_dataset, fpi_e_var_list, datestrs[0], datestrs[1], load_freq='1MS', resample_freq='1min')\n",
    "mec_data = load_range(mec_dataset, mec_var_list, datestrs[0], datestrs[1], load_freq='1MS', resample_freq='1min')\n",
    "fgm_data = load_range(fgm_dataset, fgm_var_list, datestrs[0], datestrs[1], load_freq='1D', resample_freq='1min') # load_freq is shorter here, if longer than ~10D RAM is overloaded. 1D is no slower than 10D\n",
    "swe_data = load_range(swe_dataset, swe_var_list, datestrs[0], datestrs[1], load_freq='1MS', interp_freq='1min')\n",
    "mfi_data = load_range(mfi_dataset, mfi_var_list, datestrs[0], datestrs[1], load_freq='1MS', interp_freq='1min') # MFI data is already minutely, it's just on the half-minute so we still have to interp\n",
    "\n",
    "if save_raw:\n",
    "    fpi_i_data.to_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fpi_i_1min')\n",
    "    fpi_e_data.to_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fpi_e_1min')\n",
    "    mec_data.to_hdf(DATAPATH + 'mms/mms_data.h5', key = 'mec_1min')\n",
    "    fgm_data.to_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fgm_1min')\n",
    "    swe_data.to_hdf(DATAPATH + 'wind/wind_data.h5', key = 'swe_1min')\n",
    "    mfi_data.to_hdf(DATAPATH + 'wind/wind_data.h5', key = 'mfi_1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bffcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/cobrien/tmp/ipykernel_19729/3623985841.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combo_df[key] = dataframe[key]\n",
      "/glade/derecho/scratch/cobrien/tmp/ipykernel_19729/3623985841.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combo_df[key] = dataframe[key]\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled MMS data from Toy-Edens et al. 2024 https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024JA032431\n",
    "# Downloaded from zenodo.org/records/10491878\n",
    "mms_labels = pd.read_csv(DATAPATH + 'mms/labeled_sunside_data.csv') # This object will have the MMS data from FPI, FGM, and MEC added to it.\n",
    "mms_labels = mms_labels[mms_labels['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "mms_labels['Epoch'] = pd.to_datetime(mms_labels['Epoch'], utc = True) # Change Epoch to a datetime rather than an Epoch\n",
    "mms_labels.index = mms_labels['Epoch'] # Turn the labels index into Epoch to fit it into a larger minutely dataframe to identify gaps\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the solar wind for 15 or more minutes (Used to create dataset stability flag)\n",
    "sw_regions = pd.read_csv(DATAPATH + 'mms/solar_wind_region_list.csv')\n",
    "sw_regions = sw_regions[sw_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "sw_regions['start'] = pd.to_datetime(sw_regions['start'], utc = True)\n",
    "sw_regions['stop'] = pd.to_datetime(sw_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the magnetosheath for 15 or more minutes (Used to create dataset stability flag)\n",
    "sh_regions = pd.read_csv(DATAPATH + 'mms/magnetosheath_region_list.csv')\n",
    "sh_regions = sh_regions[sh_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "sh_regions['start'] = pd.to_datetime(sh_regions['start'], utc = True)\n",
    "sh_regions['stop'] = pd.to_datetime(sh_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the magnetosphere for 15 or more minutes (Used to create dataset stability flag)\n",
    "ms_regions = pd.read_csv(DATAPATH + 'mms/magnetosphere_region_list.csv')\n",
    "ms_regions = ms_regions[ms_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "ms_regions['start'] = pd.to_datetime(ms_regions['start'], utc = True)\n",
    "ms_regions['stop'] = pd.to_datetime(ms_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the ion foreshock for 15 or more minutes (Used to create dataset stability flag)\n",
    "fs_regions = pd.read_csv(DATAPATH + 'mms/ion_foreshock_region_list.csv')\n",
    "fs_regions = fs_regions[fs_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "fs_regions['start'] = pd.to_datetime(fs_regions['start'], utc = True)\n",
    "fs_regions['stop'] = pd.to_datetime(fs_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Create a dataframe to put all the data into\n",
    "combo_df = pd.DataFrame([])\n",
    "combo_df['Epoch'] = pd.date_range(mms_labels['Epoch'].min(), mms_labels['Epoch'].max(), freq = \"1min\") # Make a time for every minute so gaps can be identified as nans\n",
    "combo_df = combo_df.loc[\n",
    "    (combo_df['Epoch'] >= pd.to_datetime(datestrs[0]))& # REMOVE THIS CUT IN PRODUCTION THIS IS JUST FOR TESTING\n",
    "    (combo_df['Epoch'] < pd.to_datetime(datestrs[1])),\n",
    "    :\n",
    "]\n",
    "combo_df.index = combo_df['Epoch'] # Make the index a time as well to simplify merging mms_labels in\n",
    "for dataframe in [mms_labels, fpi_i_data, fpi_e_data, mec_data, fgm_data, swe_data, mfi_data]:\n",
    "    dataframe.index = dataframe['Epoch']\n",
    "    for key in dataframe.columns:\n",
    "        if (key == 'Epoch'):\n",
    "            continue\n",
    "        combo_df[key] = dataframe[key]\n",
    "    dataframe = dataframe.reset_index(drop = True)\n",
    "combo_df = combo_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4d9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark entries when MMS is \"stable\" in a given region\n",
    "combo_df['stable'] = np.zeros(len(combo_df))\n",
    "for region_df in [sw_regions, sh_regions, ms_regions, fs_regions]:\n",
    "    for idx in region_df.index: # Loop over rows in the region dataframe to assess stability\n",
    "        region = region_df.loc[idx]\n",
    "        combo_df.loc[\n",
    "            (combo_df['Epoch'] >= region['start'])&\n",
    "            (combo_df['Epoch'] <= region['stop']),\n",
    "            'stable'\n",
    "        ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpi_i_data = pd.read_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fpi_i_1min')\n",
    "fpi_e_data = pd.read_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fpi_e_1min')\n",
    "mec_data = pd.read_hdf(DATAPATH + 'mms/mms_data.h5', key = 'mec_1min')\n",
    "fgm_data = pd.read_hdf(DATAPATH + 'mms/mms_data.h5', key = 'fgm_1min')\n",
    "swe_data = pd.read_hdf(DATAPATH + 'wind/wind_data.h5', key = 'swe_1min')\n",
    "mfi_data = pd.read_hdf(DATAPATH + 'wind/wind_data.h5', key = 'mfi_1min')\n",
    "\n",
    "# Load the labeled MMS data from Toy-Edens et al. 2024 https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2024JA032431\n",
    "# Downloaded from zenodo.org/records/10491878\n",
    "mms_labels = pd.read_csv(DATAPATH + 'mms/labeled_sunside_data.csv') # This object will have the MMS data from FPI, FGM, and MEC added to it.\n",
    "mms_labels = mms_labels[mms_labels['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "mms_labels['Epoch'] = pd.to_datetime(mms_labels['Epoch'], utc = True) # Change Epoch to a datetime rather than an Epoch\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the solar wind for 15 or more minutes (Used to create dataset stability flag)\n",
    "sw_regions = pd.read_csv(DATAPATH + 'mms/solar_wind_region_list.csv')\n",
    "sw_regions = sw_regions[sw_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "sw_regions['start'] = pd.to_datetime(sw_regions['start'], utc = True)\n",
    "sw_regions['stop'] = pd.to_datetime(sw_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the magnetosheath for 15 or more minutes (Used to create dataset stability flag)\n",
    "sh_regions = pd.read_csv(DATAPATH + 'mms/magnetosheath_region_list.csv')\n",
    "sh_regions = sh_regions[sh_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "sh_regions['start'] = pd.to_datetime(sh_regions['start'], utc = True)\n",
    "sh_regions['stop'] = pd.to_datetime(sh_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the magnetosphere for 15 or more minutes (Used to create dataset stability flag)\n",
    "ms_regions = pd.read_csv(DATAPATH + 'mms/magnetosphere_region_list.csv')\n",
    "ms_regions = ms_regions[ms_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "ms_regions['start'] = pd.to_datetime(ms_regions['start'], utc = True)\n",
    "ms_regions['stop'] = pd.to_datetime(ms_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data\n",
    "\n",
    "# Load just the times when MMS is \"stable\" in the ion foreshock for 15 or more minutes (Used to create dataset stability flag)\n",
    "fs_regions = pd.read_csv(DATAPATH + 'mms/ion_foreshock_region_list.csv')\n",
    "fs_regions = fs_regions[fs_regions['probe'] == 'mms1'] # Select only MMS1 so we don't overtrain\n",
    "fs_regions['start'] = pd.to_datetime(fs_regions['start'], utc = True)\n",
    "fs_regions['stop'] = pd.to_datetime(fs_regions['stop'], utc = True)  # Stops are on the 59th second of the minute so should be inclusive when binning MMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to put all the data into\n",
    "combo_df = pd.DataFrame([])\n",
    "combo_df['Epoch'] = pd.date_range(mms_labels['Epoch'].min(), mms_labels['Epoch'].max(), freq = \"1min\") # Make a time for every minute so gaps can be identified as nans\n",
    "suffixes = ['labels', '_fpi_i', '_fpi_e', '_mec', '_fgm', '_swe', '_mfi']\n",
    "for i, dataframe in enumerate([mms_labels, fpi_i_data, fpi_e_data, mec_data, fgm_data, swe_data, mfi_data]):\n",
    "    combo_df = combo_df.merge(dataframe, on = 'Epoch', suffixes=('', suffixes[i]))\n",
    "combo_df = combo_df.rename(columns = {'count' : 'count_fpi_i'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f56090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark entries when MMS is \"stable\" in a given region\n",
    "combo_df['stable'] = np.zeros(len(combo_df))\n",
    "for region_df in [sw_regions, sh_regions, ms_regions, fs_regions]:\n",
    "    for idx in region_df.index: # Loop over rows in the region dataframe to assess stability\n",
    "        region = region_df.loc[idx]\n",
    "        combo_df.loc[\n",
    "            (combo_df['Epoch'] >= region['start'])&\n",
    "            (combo_df['Epoch'] <= region['stop']),\n",
    "            'stable'\n",
    "        ] = 1\n",
    "\n",
    "# Save the combined dataframe to an HDF\n",
    "combo_df.to_hdf(DATAPATH + 'combined_data.h5', key = '1min_mms_wind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7970972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section for loading and saving OMNI supplementary data\n",
    "datestrs = ['2015-09-01 00:00:00+00:00', '2025-01-01 00:00:00+00:00']\n",
    "omni_df = load_range('OMNI_HRO_1MIN', \n",
    "                     var_list=['BX_GSE','BY_GSE','BZ_GSE',\n",
    "                               'Vx','Vy','Vz',\n",
    "                               'proton_density',\n",
    "                               'T','AE_INDEX','AL_INDEX','AU_INDEX','SYM_H', #NOTE: This line is all that we will supplement PRIME solar wind data with\n",
    "                               'BSN_x','BSN_y','BSN_z'],\n",
    "                     start_date=datestrs[0], end_date=datestrs[1], load_freq='3ME')\n",
    "omni_df.to_hdf(DATAPATH+'prime/old_prime.h5', key = 'OMNI_HRO_1MIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c358c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the old prime data\n",
    "import cdflib # Unfortunately it is saved as CDFs\n",
    "import glob # This is the easiest way to do it\n",
    "import os\n",
    "file_list = glob.glob('../../../data/prime/cdf/**/*.cdf', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Epoch', 'B_GSM', 'B_GSM_sig','V_GSE','V_GSE_sig','Ne','Ne_sig']\n",
    "test_cdf = cdflib.CDF(file_list[0])\n",
    "df_list = []\n",
    "for filename in sorted(file_list):\n",
    "    cdf = cdflib.CDF(filename)\n",
    "    df_stage = pd.DataFrame([])\n",
    "    for var in var_names:\n",
    "        if cdf[var].ndim == 2: # Vector data\n",
    "            for i in range(cdf[var].shape[-1]):\n",
    "                df_stage[var+f'_{i}'] = cdf[var][:,i]\n",
    "                df_stage[var+f'_{i}'] = df_stage[var+f'_{i}'].where(df_stage[var+f'_{i}']>-1.0E31, np.nan)\n",
    "        elif cdf[var].ndim == 1: # Scalar data\n",
    "            df_stage[var] = cdf[var]\n",
    "            df_stage[var] = df_stage[var].where(df_stage[var]>-1.0E31, np.nan)\n",
    "    df_list.append(df_stage)\n",
    "prime_df = pd.concat(df_list)\n",
    "prime_df['Epoch'] = cdflib.cdfepoch.to_datetime(prime_df['Epoch'])\n",
    "prime_df.to_hdf(DATAPATH+'prime/old_prime.h5', key = 'prime_v1_100s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data to be ingested by a SolarWind kaipy object\n",
    "datestrs = ['2015-09-01 00:00:00+00:00', '2025-01-01 00:00:00+00:00']\n",
    "prime_df = pd.read_hdf(DATAPATH+'prime/old_prime.h5', key = 'prime_v1_100s')\n",
    "\n",
    "# Resample PRIME data into a minute-cadence object\n",
    "times = pd.date_range(datestrs[0], datestrs[1], freq='1min') #Make new time intervals\n",
    "input_df = pd.DataFrame([])\n",
    "input_df['Epoch'] = times[:-1] # For some reason date_range includes the last time\n",
    "for var in prime_df.columns:\n",
    "    if (var == 'Epoch'): # Skip the time column because we already have it\n",
    "        continue\n",
    "    input_df[var] = np.interp(input_df['Epoch'], prime_df['Epoch'], prime_df[var])\n",
    "\n",
    "# Load the supplemental OMNI data \n",
    "omni_df = pd.read_hdf(DATAPATH+'prime/old_prime.h5', key = 'OMNI_HRO_1MIN')\n",
    "\n",
    "# Merge the OMNI data into the input dataframe\n",
    "input_df = input_df.merge(omni_df, on = 'Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2184049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.to_hdf(DATAPATH+'prime/old_prime.h5', key = 'prime_v1_omni_1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9c2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_hdf(DATAPATH+'prime/old_prime.h5', key = 'prime_v1_omni_1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b76aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Epoch',\n",
       " 'B_GSM_0',\n",
       " 'B_GSM_1',\n",
       " 'B_GSM_2',\n",
       " 'B_GSM_sig_0',\n",
       " 'B_GSM_sig_1',\n",
       " 'B_GSM_sig_2',\n",
       " 'V_GSE_0',\n",
       " 'V_GSE_1',\n",
       " 'V_GSE_2',\n",
       " 'V_GSE_sig_0',\n",
       " 'V_GSE_sig_1',\n",
       " 'V_GSE_sig_2',\n",
       " 'Ne',\n",
       " 'Ne_sig',\n",
       " 'BX_GSE',\n",
       " 'BY_GSE',\n",
       " 'BZ_GSE',\n",
       " 'Vx',\n",
       " 'Vy',\n",
       " 'Vz',\n",
       " 'proton_density',\n",
       " 'T',\n",
       " 'AE_INDEX',\n",
       " 'AL_INDEX',\n",
       " 'AU_INDEX',\n",
       " 'SYM_H',\n",
       " 'BSN_x',\n",
       " 'BSN_y',\n",
       " 'BSN_z']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(input_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae148a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt212gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
